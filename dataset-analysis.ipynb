{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset analysis\n",
    "\n",
    "Recognition of car traffic lights based on visual observation using convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries\n",
    "\n",
    "Configuration of the development environment by loading the appropriate libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np                      # Support for linear algebra\n",
    "import pandas as pd                     # Support for working with data and support for loading data from CSV files\n",
    "import duplicates                       # Search for duplicate files\n",
    "import cv2                              # OpenCV library for working with graphic files \n",
    "from PIL import Image                   # An image class from the PIL library, an alternative to OpenCV\n",
    "import os                               # Access to operating system functions (including access to the file system)\n",
    "from matplotlib import pyplot as plt    # Support for drawing graphs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 314159  # Seed for algorithms that require randomness to produce reproducible results\n",
    "\n",
    "LABEL_FILE = \"Bochum.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABEL_FILE, \"r\") as fp:\n",
    "    images = json.load(fp)\n",
    "\n",
    "files_count = 0\n",
    "image_paths = []\n",
    "label_dfs = []\n",
    "\n",
    "for image in images[\"images\"]:\n",
    "\n",
    "    image_paths.append(image['image_path'])\n",
    "\n",
    "    files_count += 1\n",
    "\n",
    "    df = pd.DataFrame(image[\"labels\"])\n",
    "    df = df.drop('attributes', axis=1).assign(**pd.DataFrame(df.attributes.values.tolist()))\n",
    "    label_dfs.append(df)\n",
    "\n",
    "label_df = pd.concat(label_dfs)\n",
    "\n",
    "print(\"File count: \", files_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying the properties of a dataset\n",
    "\n",
    "Once you have loaded the annotation data, you need to analyze the properties of the information available. Good knowledge of the data set will allow you to make the right decisions when designing the detector and will allow for an in-depth understanding of the problem. Conclusions from the entire process can be found at the end of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Search for missing values\n",
    "\n",
    "The first step in examining the training set is to search the columns for missing values. In many datasets, it happens that individual values are missing in different columns. For example, in a single-family home information set, the \"Garage Size\" column may not have a value for some homes. This may mean no garage in this case. This will allow us to determine if additional data preprocessing steps, such as injecting values, will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empty values\")\n",
    "display(label_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check for duplicate files\n",
    "\n",
    "The presence of duplicates in datasets should also be investigated. In order to ensure appropriate conditions for the learning process, it is important to ensure that all samples are unique. For this purpose, we will use the external library `Duplicate Finder` available at https://github.com/akcarsten/Duplicate-Finder. This library uses the SHA256 algorithm to compute a unique hash value for each file. The probability of two files colliding is so small that it is negligible in practice.\n",
    "\n",
    "From the results, repetitions related to the existence of catalogs with sample sequences should be filtered out. The authors of the dataset simply copied individual sequences with their dedicated annotations and placed them in a common directory. The name of these directories starts with the `sample` prefix, which will make it easier to filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a list of all duplicate files with their hashes\n",
    "all_duplicates = duplicates.list_all_duplicates(LABEL_FILE.replace(\".json\", \"\"), ext=\".tiff\", fastscan=True)\n",
    "\n",
    "print()\n",
    "print(f'Number of duplicates = {all_duplicates[\"hash\"].nunique()}')\n",
    "print(\"Duplicates:\")\n",
    "print(all_duplicates[\"file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Study of frame properties\n",
    "\n",
    "When using convolutional neural networks, make sure that the dimensions of all images fed into the network input are the same. To do this, we will iterate through all the graphics available in the dataset and collect their properties in the Pandas table. We will use the PIL library, which when opening an image file does not immediately load it into memory, which will significantly speed up the whole process. The image parameters that we will pay special attention to are:\n",
    "\n",
    "- width in pixels\n",
    "\n",
    "- height in pixels\n",
    "\n",
    "- graphic file format\n",
    "\n",
    "- number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_properties_dict = {\"name\": [], \"width\": [], \"height\": [], \"format\": [], \"mode\": []}\n",
    "\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    name = os.path.basename(image_path)\n",
    "    img_properties_dict[\"name\"].append(name)\n",
    "    img_properties_dict[\"width\"].append(img.width)\n",
    "    img_properties_dict[\"height\"].append(img.height)\n",
    "    img_properties_dict[\"format\"].append(img.format)\n",
    "    img_properties_dict[\"mode\"].append(img.mode)\n",
    "\n",
    "img_properties = pd.DataFrame(img_properties_dict)\n",
    "\n",
    "print(\"Counting unique frame parameters:\")\n",
    "img_properties_groups = img_properties.drop([\"name\"], axis=1).value_counts()\n",
    "display(img_properties_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. State Probability Distribution Study\n",
    "\n",
    "The next important step is to examine the number of states (color of the traffic light) and their portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = label_df[\"state\"].unique()\n",
    "print(\"Traffic light states:\")\n",
    "print(states)\n",
    "\n",
    "states_count = label_df[\"state\"].value_counts()\n",
    "states_freq = label_df[\"state\"].value_counts(normalize=True)\n",
    "states_params = pd.DataFrame({\"Count\": states_count, \"Frequency\": states_freq})\n",
    "print(\"Probability distribution and state size:\")\n",
    "display(states_params)\n",
    "\n",
    "states_count.plot(\n",
    "    kind=\"bar\",\n",
    "    title=f'Numbers of individual states of traffic lights in the dataset',\n",
    "    rot=45,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pictogram Probability Distribution Study\n",
    "\n",
    "The next important step is to examine the number of pictograms and their portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pictogram = label_df[\"pictogram\"].unique()\n",
    "print(\"Traffic light pictograms:\")\n",
    "print(pictogram)\n",
    "\n",
    "pictogram_count = label_df[\"pictogram\"].value_counts()\n",
    "pictogram_freq = label_df[\"pictogram\"].value_counts(normalize=True)\n",
    "pictogram_params = pd.DataFrame({\"Count\": pictogram_count, \"Frequency\": pictogram_freq})\n",
    "print(\"Probability distribution and size:\")\n",
    "display(pictogram_params)\n",
    "\n",
    "pictogram_count.plot(\n",
    "    kind=\"bar\",\n",
    "    title=f'Numbers of individual pictograms of traffic lights in the dataset',\n",
    "    rot=45,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Direction Probability Distribution Study\n",
    "\n",
    "The dataset also contains label data for traffic lights which are seen from behind or the side. These instances might not provide useful references for our model so it would be wise to see how big of a portion of the data set they are.\n",
    "\n",
    "(TODO??) see how many traffic lights with direction 'back' have state 'off' or 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = label_df[\"direction\"].unique()\n",
    "print(\"Traffic light directions:\")\n",
    "print(directions)\n",
    "\n",
    "directions_count = label_df[\"direction\"].value_counts()\n",
    "directions_freq = label_df[\"direction\"].value_counts(normalize=True)\n",
    "directions_params = pd.DataFrame({\"Count\": directions_count, \"Frequency\": directions_freq})\n",
    "print(\"Probability distribution and size:\")\n",
    "display(directions_params)\n",
    "\n",
    "directions_count.plot(\n",
    "    kind=\"bar\",\n",
    "    title=f'Numbers of individual directions of traffic lights in the dataset',\n",
    "    rot=45,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Displaying sample frames with annotations\n",
    "\n",
    "The last step of the analysis is to visually familiarize ourselves with the contents of the DriveU Traffic Light Dataset. This does not add significant information to the analysis process, but it allows you to better imagine the problem we are working with.\n",
    "\n",
    "We will draw a few random frames along with the labeling of the traffic light. Each state will be marked with a different color according to the table below.\n",
    "\n",
    "| State | Color |\n",
    "|---------------|-|\n",
    "| `green` | green |\n",
    "| `red` | red |\n",
    "| `red_yellow` | orange |\n",
    "| `yellow` | yellow |\n",
    "| `off` | black |\n",
    "| `unknown` | grey |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map states to \n",
    "STATE_TO_COLOR = {\n",
    "    \"green\": (0, 255, 0),  # Green\n",
    "    \"red\": (0, 0, 255),  # Red\n",
    "    \"red_yellow\": (0, 127, 255),  # Orange\n",
    "    \"yellow\": (0, 255, 255),  # Yellow\n",
    "    \"off\": (0, 0, 0),  # Black\n",
    "    \"unknown\": (169,169,169),  # Grey\n",
    "}\n",
    "\n",
    "def print_annotated_img(image_path, annotations, state_to_color_map):\n",
    "    \"\"\"\n",
    "    Funkcja, która wyrysowyje obrazek ze zbioru danych\n",
    "    wraz z ramkami opisującymi sygnalizatory\n",
    "\n",
    "    Parametry:\n",
    "    image_path - pełna ścieżka do obrazka\n",
    "    annotations - tabela z adnotacjami dot. danego obrazka zawierająca jedną\n",
    "                  kolumnę z klasą obiektu i cztery kolumny opisujące współrzędne\n",
    "                  ramki\n",
    "    state_to_color_map - słownik mapujący daną klasę na kolor w palecie RGB\n",
    "\n",
    "    Wartość zwracana:\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Load image from file path, do debayering and shift\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    if os.path.splitext(image_path)[1] == \".tiff\":\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BAYER_GB2BGR)\n",
    "        # Images are saved in 12 bit raw -> shift 4 bits\n",
    "        img = np.right_shift(img, 4)\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "    box_thickness = 2\n",
    "    for idx, row in annotations.iterrows():\n",
    "        box_color = state_to_color_map[row[\"state\"]]\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (row[\"x\"], row[\"y\"]),\n",
    "            (row[\"x\"] + row[\"w\"], row[\"y\"] + row[\"h\"]),\n",
    "            box_color,\n",
    "            box_thickness,\n",
    "        )\n",
    "    \n",
    "    img = img[..., ::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(image_path)\n",
    "    ax.imshow(img)\n",
    "\n",
    "# Pick one picture for each state\n",
    "for light_state in label_df[\"state\"].unique():\n",
    "    sample_file = random.choice(images[\"images\"])\n",
    "    sample_annotations = pd.DataFrame(sample_file[\"labels\"])\n",
    "    sample_annotations = sample_annotations.drop('attributes', axis=1).assign(**pd.DataFrame(sample_annotations.attributes.values.tolist()))\n",
    "    \n",
    "    print_annotated_img(sample_file['image_path'], sample_annotations, STATE_TO_COLOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observations and conclusions\n",
    "\n",
    "The following list contains all the observations and conclusions based on the results obtained in the other sections of this document:\n",
    "\n",
    "- The labels in the data set exhibit no instances of missing or empty values, indicating a high level of data completeness. Thus additional data preprocessing operations will not be needed.\n",
    "\n",
    "- There are 40978 frames in total. This is a very large sample, which has a positive effect on the quality of training the neural network.\n",
    "\n",
    "- The dataset does not contain duplicate frames, so there is no chance of overrepresentation of individual features of specific images.\n",
    "\n",
    "- The images within the dataset all share consistent properties. They have dimensions of 2048×1024 pixels & TIFF file format. REMOVE -> ???The size of the graphics is quite large considering the fact that they will be used in the process of teaching the neural network. This will have a negative impact on the time of network training as well as on the consumption of hardware resources (especially the memory of the graphics processor).\n",
    "\n",
    "- The most prevelant traffic light states (excluding 'unknown') are 'green' and 'red'. There are several times more of them than each of the other states and in total they constitute over 60% of the data set. This distribution can potentially impact the performance of any model trained on this data set, especially due to the imbalance between green and yellow states.\n",
    "\n",
    "- The 'unknown' traffic light state makes up 27% of all data points, which could potentially harm the reliability of the model as these traffic lights might not be properly visible or photographed from an angle that obstructs their visibility.\n",
    "\n",
    "- 'yellow', 'red_yellow' & 'off' have a frequency of less than 10%. Underrepresentation of these states may prevent us from achieving a sufficiently good precision in detecting them. Despite the large amount of training data, there is a suspicion that the detection efficiency of these signals may not be sufficient for the network to be used in real-life conditions, where almost 100% is required in every case.\n",
    "\n",
    "- Due to the fact that the data set is based on sequential recordings, and not on random photos, the distribution of states in our dataset reflects real-world traffic light dynamics and provides insight into their usage patterns. For example, a warning light (yellow) is lit much shorter than a red or green light, and this fact is reflected in the shortage of the `yellow` and `red_yellow` states within our data.\n",
    "\n",
    "- The most prevelant traffic light pictograms (excluding 'unknown') are 'circle' (43%) and 'pedestrian' (20%). This distribution can have a big impact on the performance of any model trained on this data set.\n",
    "\n",
    "- 'tram', 'arrow_straight', 'bicycle', 'pedestrian_bicycle', 'arrow_right', 'arrow_straight_left' are all less than 5% of the data set, which can lead to dificulties with the recognition of such traffic lights.\n",
    "\n",
    "(TODO?? see if train & test data seperately have good distribution)\n",
    "Based on this research, we can make a final decision on how to work with the dataset. The division into train and test subsets can be used as proposed by the authors of the data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
